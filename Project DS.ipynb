{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "d85ae62a-df68-4ed1-8c49-6ef6a4495ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "from collections import Counter\n",
    "import spacy\n",
    "import os\n",
    "import torch.optim as optim\n",
    "import re\n",
    "import contractions\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_parquet = '/Users/colinjohnson/Documents/code/GitHub/DS-5110/train_data.parquet'\n",
    "test_parquet = '/Users/colinjohnson/Documents/code/GitHub/DS-5110/test_data.parquet'\n",
    "vocab_file = '/Users/colinjohnson/Documents/code/GitHub/DS-5110/aclImdb/imdb.vocab'\n",
    "\n",
    "df_test = pd.read_parquet(test_parquet, engine='pyarrow') \n",
    "df_train = pd.read_parquet(train_parquet, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "55170bf5-e439-464e-86bc-5998740122a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Find vocab file and create a dictionary of all the words\n",
    "'''\n",
    "\n",
    "vocab = {}\n",
    "with open(vocab_file, 'r',encoding='latin-1') as file:\n",
    "    for idx, line in enumerate(file, start=1): \n",
    "        word = line.strip()  \n",
    "        vocab[word] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "5c35c403-d360-4605-9e32-21b2655aed3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Preprocess data removing tags, lemmanizing the data\n",
    "\n",
    "'''\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "def remove_html_tags(text):\n",
    "    return re.sub(r\"<.*?>\", \"\", text)\n",
    "def spacy_lemmatization(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.lemma_ for token in doc]\n",
    "def filter_vocab(vocab, words_remove):\n",
    "    #filter, function applied in tokenizing the vocab\n",
    "    return {word for word in vocab if word not in words_remove}\n",
    "def tokenize_with_vocab(text, vocab):\n",
    "    '''\n",
    "    Tokenizing the reviews, removing common and useless words, etc.\n",
    "    '''\n",
    "    words_remove = ['movie', 'film', 'director', 'plays', 'horror', 'comedy', 'watching', 'seen', 'people', 'guy', 's', 'time', 'second', 'book']\n",
    "    vocab = filter_vocab(vocab, words_remove)\n",
    "    tokenized_reviews = []\n",
    "    for doc in nlp.pipe(text, batch_size=1000, disable=[\"parser\", \"ner\", \"tagger\"]):\n",
    "        cleaned_text = remove_html_tags(doc.text)\n",
    "        lemmas = spacy_lemmatization(cleaned_text)\n",
    "        tokens = [\n",
    "            lemma.lower() for lemma in lemmas\n",
    "            if lemma.lower() in vocab and lemma.lower() not in nlp.Defaults.stop_words\n",
    "        ]\n",
    "        tokenized_reviews.append(tokens)\n",
    "    \n",
    "    return tokenized_reviews\n",
    "def tokenize_id(tokens, vocab):\n",
    "    '''\n",
    "    Retrieving the id values of the tokens corresponding with the vocab dicitionary\n",
    "    '''\n",
    "    return [vocab[token] for token in tokens]\n",
    "def pad_sequences(sequences, max_length):\n",
    "    '''\n",
    "    Padding data so each review length is equal, necessary for the model\n",
    "    '''\n",
    "    return rnn_utils.pad_sequence(sequences, batch_first=True, padding_value=0)[:, :max_length]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "a2e38d23-717e-4e87-b2c1-2f38602aa15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n",
      "/opt/anaconda3/lib/python3.12/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Application of the previous various preprocessing tasks on the train & test dataset\n",
    "'''\n",
    "df_train['tokenized_review'] = tokenize_with_vocab(df_train['review'].values.tolist(), vocab)\n",
    "df_test['tokenized_review'] = tokenize_with_vocab(df_test['review'].values.tolist(), vocab)\n",
    "df_train['review_ids'] = df_train['tokenized_review'].apply(lambda tokens: tokenize_id(tokens, vocab))\n",
    "df_test['review_ids'] = df_test['tokenized_review'].apply(lambda tokens: tokenize_id(tokens, vocab))\n",
    "df_train['review_length'] = df_train['tokenized_review'].apply(len)\n",
    "df_test['review_length'] = df_test['tokenized_review'].apply(len)\n",
    "\n",
    "\n",
    "#To prevent overfitting we pad to a length that is in the 90% percentile in length of all the review lengths\n",
    "max_length = int(df_train['review_length'].quantile(.9))\n",
    "\n",
    "sequence_train = [torch.tensor(ids, dtype=torch.long) for ids in df_train['review_ids']]\n",
    "sequence_test = [torch.tensor(ids, dtype=torch.long) for ids in df_test['review_ids']]\n",
    "\n",
    "train_padded = pad_sequences(sequence_train, max_length)\n",
    "test_padded = pad_sequences(sequence_test, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "9290b98f-9840-4ea6-9d27-745454bb20e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, conv_config, output_size, dropout=0.5):\n",
    "        super(TextCNN, self).__init__() \n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.conv_config = conv_config\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.convolutions = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv1d(embedding_dim, self.conv_config['num_channels'], kernel_size=kernel),\n",
    "                nn.ReLU(),\n",
    "                nn.AdaptiveMaxPool1d(1)\n",
    "            )\n",
    "            for kernel in self.conv_config['kernel_sizes']\n",
    "        ])\n",
    "\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.linear = nn.Linear(\n",
    "            self.conv_config['num_channels'] * len(self.conv_config['kernel_sizes']),\n",
    "            self.output_size\n",
    "        )\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        \"\"\"\n",
    "        Forward pass for the TextCNN model.\n",
    "        Args:\n",
    "            input_seq (Tensor): Input tensor with shape (batch_size, seq_length).\n",
    "        Returns:\n",
    "            Tensor: Log-softmax probabilities with shape (batch_size, output_size).\n",
    "        \"\"\"\n",
    "        \n",
    "        emb_out = self.embedding(input_seq).permute(0, 2, 1)  \n",
    "\n",
    "\n",
    "        conv_out = [conv(emb_out).squeeze(2) for conv in self.convolutions]\n",
    "\n",
    "        concat_out = torch.cat(conv_out, dim=1)\n",
    "\n",
    "        concat_out = self.dropout(concat_out)\n",
    "        out = self.linear(concat_out)\n",
    "\n",
    "        return F.log_softmax(out, dim=-1)\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = sequences\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "bc077e32-bed1-447e-acdb-386540acdc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs=5):\n",
    "    device='cpu'\n",
    "    model = model.to(device)\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "    best_acc = 0.0  \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = 100. * correct / total\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(epoch_acc)\n",
    "\n",
    "       \n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                test_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        \n",
    "        test_loss = test_loss / len(test_loader)\n",
    "        test_acc = 100. * correct / total\n",
    "        test_losses.append(test_loss)\n",
    "        test_accuracies.append(test_acc)\n",
    "        \n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.2f}%')\n",
    "        print(f'Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n",
    "        print(f'Best Test Acc: {best_acc:.2f}%')\n",
    "        print('-' * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7396176-f3f9-4019-b0f5-acf1f3563da8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conv_config = {'num_channels': 50, 'kernel_sizes': [1, 2, 3]}\n",
    "output_size = 2\n",
    "learning_rate = 0.001\n",
    "dropout = 0.8\n",
    "embedding_dim = 128\n",
    "\n",
    "vocab_size = len(vocab) + 1  \n",
    "\n",
    "train_labels = df_train['label'].values\n",
    "test_labels = df_test['label'].values\n",
    "\n",
    "train_dataset = SentimentDataset(train_padded, train_labels)\n",
    "test_dataset = SentimentDataset(test_padded, test_labels]\n",
    "                                \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "CUDA = torch.cuda.is_available()\n",
    "\n",
    "model = TextCNN(vocab_size, embedding_dim, conv_config, output_size, dropout)\n",
    "\n",
    "if CUDA:\n",
    "    model = model.cuda()\n",
    "\n",
    "print(model)\n",
    "\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "\n",
    "train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs=num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443d6ee7-6780-4e8a-b917-85cbfb9f1509",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcb04a5-6195-44e0-9bb7-bad2900265c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
